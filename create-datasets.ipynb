{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of train/val/test datasets used in `tutorial-segmentation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and function definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    " \n",
    "%matplotlib inline\n",
    "\n",
    "def get_section_no(name):\n",
    "    if isinstance(name, list):\n",
    "        res = []\n",
    "        for n in name:\n",
    "            res.append(get_section_no(n))\n",
    "        return res\n",
    "    else:\n",
    "        return re.findall(r'\\d\\d\\d\\d', name)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly crop patches from train and val sections\n",
    "def crop_patches(imgs, masks, coords, patch_size=(256,256), label_map=None):\n",
    "    \"\"\"Randomly crop patches of patch_size from imgs. \n",
    "    Ensures that entire patch is inside image\n",
    "    \n",
    "    Args:\n",
    "        imgs, masks: np.array of shape (num_imgs, h, w) containing images and masks to be cropped\n",
    "        coords: np.array of shape (num, 3), upper left coordinate of patches that should be cropped\n",
    "        patch_size: shape of resulting patches\n",
    "        label_map: dictionary mapping values in masks to certain labels \n",
    "    Returns:\n",
    "        patches, labels: np.array of shape (num_patches, patch_size)\"\"\"\n",
    "    \n",
    "    # crop patches from imgs and masks for each selected coordinate\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for coord in tqdm(coords):\n",
    "        slice_h = slice(coord[1], coord[1]+patch_size[0])\n",
    "        slice_w = slice(coord[2], coord[2]+patch_size[1])\n",
    "        patches.append(imgs[coord[0],slice_h,slice_w])\n",
    "        if label_map is not None:\n",
    "            label = np.zeros(patch_size, dtype=np.uint8)\n",
    "            for k,v in label_map.items():\n",
    "                label[masks[coord[0],slice_h,slice_w]==k] = v\n",
    "        else:\n",
    "            label = masks[coord[0],slice_h, slice_w]\n",
    "        labels.append(label)\n",
    "    return patches, labels\n",
    "\n",
    "def sample_coordinates(imgs, num, patch_size=(256,256), p=None):\n",
    "    \"\"\"\n",
    "    Sample num coordinates from imgs\n",
    "    Coordinates represent upper left corner of patch of size patch_size. Ensures that entire patch is inside imgs\n",
    "    Args:\n",
    "        imgs: np.array of shape (num_imgs, h, w)\n",
    "        num: int, number of coordinates that should be sampled\n",
    "        p: array of shape (num_imgs, h, w), default is None. Sampling probability for each coordinate\n",
    "    Returns:\n",
    "        coords: np.array of shape (num, 3), sampled coordinates\n",
    "    \"\"\"\n",
    "    # select coordinates of upper left corner of patch\n",
    "    # entire patch should fit in image\n",
    "    shape = [imgs.shape[0], imgs.shape[1]-patch_size[0], imgs.shape[2]-patch_size[1]] \n",
    "    if p is not None:\n",
    "        # remove borders from p array, such that elements correspond to sampling probability of center pixel of patch\n",
    "        slice_h = slice(patch_size[0]//2, p.shape[1]-patch_size[0]//2)\n",
    "        slice_w = slice(patch_size[1]//2, p.shape[2]-patch_size[1]//2)\n",
    "        p = p[:,slice_h,slice_w]\n",
    "        coords = weighted_random_choice(num=num, weights=p) # uses np.random.choice\n",
    "    else: # random sampling of coordinates\n",
    "        indices = np.random.randint(low=0, high=shape[0]*shape[1]*shape[2], size=num)\n",
    "        coords = np.unravel_index(indices, shape)\n",
    "        coords = np.array(coords).T  # coords has shape num, 3\n",
    "    return coords\n",
    "\n",
    "def weighted_random_choice(num, weights):\n",
    "    \"\"\"\n",
    "    Sample num coordinates, respecting the given sampling weights\n",
    "    \"\"\"\n",
    "    # First, choose how many values to sample along axis 1\n",
    "    i1 = np.random.choice(range(0, weights.shape[0]), size=num, p=np.sum(weights, axis=(1,2))/np.sum(weights))\n",
    "    inds, nums = np.unique(i1, return_counts=True)\n",
    "    coords = []\n",
    "    for i, num in tqdm(zip(inds, nums)):\n",
    "        #print('Sampling', num, 'from axis', i)\n",
    "        # choose num coordinates from weights[i]\n",
    "        ind = np.random.choice(range(0, weights.shape[1]*weights.shape[2]), size=num, p=weights[i].flatten()/np.sum(weights[i]))\n",
    "        for c in np.array(np.unravel_index(ind, weights.shape[1:])).T:\n",
    "            coords.append([i, c[0], c[1]])\n",
    "    # shuffle coords\n",
    "    np.random.shuffle(coords)\n",
    "    return np.array(coords)\n",
    "    \n",
    "def get_sampling_weights(masks, label_vals):\n",
    "    \"\"\"\n",
    "    Return matrix with sampling probability for each element in masks, \n",
    "    such that each label in masks will be sampled with equal probability\n",
    "    Args:\n",
    "        masks: array containing labels\n",
    "        label_vals: list of label values that should be considered in masks\n",
    "    \"\"\"\n",
    "    # weight matrix, weighing gm/wm/background equally (to ensure equal sampling)\n",
    "    weights = np.zeros(masks.shape)\n",
    "    for l in label_vals:\n",
    "        binary_mask = masks == l\n",
    "        p = 1./np.sum(binary_mask)\n",
    "        weights[binary_mask] = p\n",
    "    weights = weights / np.sum(weights)\n",
    "    return weights\n",
    "\n",
    "def calculate_patch_dataset(sections, sections_fname, masks_fname, num_patches, label_map, patch_size):\n",
    "    # load images\n",
    "    print(\"Loading images\")\n",
    "    masks = np.array([imageio.imread(masks_fname.format(sec)) for sec in sections])\n",
    "    imgs = np.array([imageio.imread(sections_fname.format(sec)) for sec in sections])\n",
    "    # ensure equal sampling of all classes\n",
    "    print(\"Calculating sampling_weights\")\n",
    "    sampling_weights = get_sampling_weights(masks, label_vals=label_map.keys()) \n",
    "    #sampling_weights = None\n",
    "    # randomly sampled coordinates\n",
    "    print(\"Sampling coordinates\")\n",
    "    coords = sample_coordinates(imgs, num_patches, patch_size=patch_size, p=sampling_weights)\n",
    "    # crop patches\n",
    "    print(\"Cropping patches from coordinates\")\n",
    "    patches, labels = crop_patches(imgs, masks, coords, patch_size=patch_size, label_map=label_map)\n",
    "\n",
    "    # prepare for use as training dataset\n",
    "    X = np.expand_dims(np.array(patches), 3)\n",
    "    Y = np.expand_dims(np.array(labels), 3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 masks downloaded to data/raw/masks_v1\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/raw'\n",
    "\n",
    "# download V1 masks\n",
    "masks_v1_url = 'https://object.cscs.ch/v1/AUTH_227176556f3c4bb38df9feea4b91200c/hbp-d002272_BigBrainCytoMapping-v1-v2_pub/ReferenceDelineations/v1/2019_05_01_v1.zip'\n",
    "masks_v1_archive = os.path.join(data_dir, 'masks_v1.zip')\n",
    "masks_v1_dir = os.path.join(data_dir, 'masks_v1')\n",
    "!mkdir -p {data_dir}\n",
    "!wget -q -nc {masks_v1_url} -O {masks_v1_archive}\n",
    "!unzip -qo {masks_v1_archive} -d {masks_v1_dir}\n",
    "!mv {masks_v1_dir}/v1/* {masks_v1_dir}/\n",
    "!rmdir {masks_v1_dir}/v1\n",
    "print('V1 masks downloaded to', masks_v1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 BigBrain sections downloaded to data/raw/sections\n"
     ]
    }
   ],
   "source": [
    "# download BigBrain sections for every mask\n",
    "sections_url = 'ftp://bigbrain.loris.ca/BigBrainRelease.2015/2D_Final_Sections/Coronal/Png/Full_Resolution/pm{}o.png'\n",
    "sections_dir = os.path.join(data_dir, 'sections')\n",
    "sections_fname = os.path.join(sections_dir, 'B20_{}.png')\n",
    "!mkdir -p {sections_dir}\n",
    "num = 0\n",
    "for f in glob.glob(os.path.join(masks_v1_dir, '*')):\n",
    "    section_no = get_section_no(f)\n",
    "    !wget -q -nc {sections_url.format(section_no)} -O {sections_fname.format(section_no)}\n",
    "    num += 1\n",
    "print(num, 'BigBrain sections downloaded to', sections_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gray/white matter masks downloaded to data/raw/masks_gmwm\n"
     ]
    }
   ],
   "source": [
    "# download gm/wm segmentations (sliced from segmented volume)\n",
    "masks_gmwm_url = 'https://fz-juelich.sciebo.de/s/YwSi8flpJVtVsKD/download'\n",
    "masks_gmwm_archive = os.path.join(data_dir, 'masks_gmwm.zip')\n",
    "masks_gmwm_dir = os.path.join(data_dir, 'masks_gmwm')\n",
    "!mkdir -p {data_dir}\n",
    "!wget -q -nc {masks_gmwm_url} -O {masks_gmwm_archive}\n",
    "!unzip -qo {masks_gmwm_archive} -d {masks_gmwm_dir} \n",
    "!mv {masks_gmwm_dir}/masks_gmwm/* {masks_gmwm_dir}/\n",
    "!rmdir {masks_gmwm_dir}/masks_gmwm\n",
    "print('Gray/white matter masks downloaded to', masks_gmwm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw:\r\n",
      "masks_gmwm  masks_gmwm.zip  masks_v1  masks_v1.zip  sections\r\n",
      "\r\n",
      "data/raw/masks_gmwm:\r\n",
      "B20_0061_gmwm.png  B20_0661_gmwm.png  B20_1261_gmwm.png  B20_1861_gmwm.png\r\n",
      "B20_0121_gmwm.png  B20_0721_gmwm.png  B20_1321_gmwm.png  B20_1921_gmwm.png\r\n",
      "B20_0181_gmwm.png  B20_0781_gmwm.png  B20_1381_gmwm.png  B20_1980_gmwm.png\r\n",
      "B20_0241_gmwm.png  B20_0841_gmwm.png  B20_1441_gmwm.png  B20_2041_gmwm.png\r\n",
      "B20_0301_gmwm.png  B20_0901_gmwm.png  B20_1501_gmwm.png  B20_2101_gmwm.png\r\n",
      "B20_0361_gmwm.png  B20_0961_gmwm.png  B20_1561_gmwm.png  B20_2161_gmwm.png\r\n",
      "B20_0421_gmwm.png  B20_1021_gmwm.png  B20_1621_gmwm.png  B20_2221_gmwm.png\r\n",
      "B20_0481_gmwm.png  B20_1081_gmwm.png  B20_1681_gmwm.png  B20_2281_gmwm.png\r\n",
      "B20_0541_gmwm.png  B20_1141_gmwm.png  B20_1741_gmwm.png  B20_2341_gmwm.png\r\n",
      "B20_0601_gmwm.png  B20_1201_gmwm.png  B20_1801_gmwm.png\r\n",
      "\r\n",
      "data/raw/masks_v1:\r\n",
      "B20_0061_v1.png  B20_0661_v1.png  B20_1261_v1.png  B20_1861_v1.png\r\n",
      "B20_0121_v1.png  B20_0721_v1.png  B20_1321_v1.png  B20_1921_v1.png\r\n",
      "B20_0181_v1.png  B20_0781_v1.png  B20_1381_v1.png  B20_1980_v1.png\r\n",
      "B20_0241_v1.png  B20_0841_v1.png  B20_1441_v1.png  B20_2041_v1.png\r\n",
      "B20_0301_v1.png  B20_0901_v1.png  B20_1501_v1.png  B20_2101_v1.png\r\n",
      "B20_0361_v1.png  B20_0961_v1.png  B20_1561_v1.png  B20_2161_v1.png\r\n",
      "B20_0421_v1.png  B20_1021_v1.png  B20_1621_v1.png  B20_2221_v1.png\r\n",
      "B20_0481_v1.png  B20_1081_v1.png  B20_1681_v1.png  B20_2281_v1.png\r\n",
      "B20_0541_v1.png  B20_1141_v1.png  B20_1741_v1.png  B20_2341_v1.png\r\n",
      "B20_0601_v1.png  B20_1201_v1.png  B20_1801_v1.png\r\n",
      "\r\n",
      "data/raw/sections:\r\n",
      "B20_0061.png  B20_0541.png  B20_1021.png  B20_1501.png\tB20_1980.png\r\n",
      "B20_0121.png  B20_0601.png  B20_1081.png  B20_1561.png\tB20_2041.png\r\n",
      "B20_0181.png  B20_0661.png  B20_1141.png  B20_1621.png\tB20_2101.png\r\n",
      "B20_0241.png  B20_0721.png  B20_1201.png  B20_1681.png\tB20_2161.png\r\n",
      "B20_0301.png  B20_0781.png  B20_1261.png  B20_1741.png\tB20_2221.png\r\n",
      "B20_0361.png  B20_0841.png  B20_1321.png  B20_1801.png\tB20_2281.png\r\n",
      "B20_0421.png  B20_0901.png  B20_1381.png  B20_1861.png\tB20_2341.png\r\n",
      "B20_0481.png  B20_0961.png  B20_1441.png  B20_1921.png\r\n"
     ]
    }
   ],
   "source": [
    "# look at structure of data directory\n",
    "!ls -R {data_dir}\n",
    "\n",
    "sections_fname = os.path.join(data_dir, 'sections/B20_{}.png')\n",
    "masks_v1_fname = os.path.join(data_dir, 'masks_v1/B20_{}_v1.png')\n",
    "masks_gmwm_fname = os.path.join(data_dir, 'masks_gmwm/B20_{}_gmwm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sample train/val patches from sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train/val/test images: 26/7/6\n",
      "Train sections ['0061', '0121', '0241', '0301', '0421', '0481', '0601', '0661', '0781', '0841', '0961', '1021', '1141', '1201', '1321', '1381', '1501', '1561', '1681', '1741', '1861', '1921', '2041', '2101', '2221', '2281']\n",
      "Val sections ['0181', '0541', '0901', '1261', '1621', '1980', '2341']\n",
      "Test sections ['0361', '0721', '1081', '1441', '1801', '2161']\n"
     ]
    }
   ],
   "source": [
    "# split available sections in train/val and test sections\n",
    "imgs = sorted(glob.glob(sections_fname.format('*')))\n",
    "train_imgs = []\n",
    "test_imgs = []\n",
    "val_imgs = []\n",
    "for i, img in enumerate(imgs):\n",
    "    if i%6 in (0,1,3,4):\n",
    "        train_imgs.append(img)\n",
    "    elif i%6 == 2:\n",
    "        val_imgs.append(img)\n",
    "    else:\n",
    "        test_imgs.append(img)\n",
    "print('Number of train/val/test images: {}/{}/{}'.format(len(train_imgs),len(val_imgs),len(test_imgs)))\n",
    "\n",
    "train_sections = get_section_no(train_imgs)\n",
    "val_sections = get_section_no(val_imgs)\n",
    "test_sections = get_section_no(test_imgs)\n",
    "print('Train sections', train_sections)\n",
    "print('Val sections', val_sections)\n",
    "print('Test sections', test_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test sections in extra folder\n",
    "test_dir = 'data/test'\n",
    "!mkdir -p {test_dir}\n",
    "for s in test_sections:\n",
    "    !cp {sections_fname.format(s)} {os.path.join(test_dir, os.path.basename(sections_fname.format(s)))}\n",
    "    !cp {masks_gmwm_fname.format(s)} {os.path.join(test_dir, os.path.basename(masks_gmwm_fname.format(s)))}\n",
    "    !cp {masks_v1_fname.format(s)} {os.path.join(test_dir, os.path.basename(masks_v1_fname.format(s)))}\n",
    "!tar -cf data/test.tar {test_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc258d646c2d4f39ba3f39c9080e3a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1b2181e16b4bf6af72039f0b57c92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd24e001f4c143b6beccaf4490a05719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a303b7c81f49daa16a8e12b8a48767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- caution, calculating weights and datasets needs around 20GB of RAM\n",
    "# -- if this step runs out of memory, set weights = None (random sampling of pixels), or use precomputed patches\n",
    "np.random.seed(42) # get reproducible datasets\n",
    "# sample training dataset for gray/white matter segmentation\n",
    "num_train = 500\n",
    "X_train, Y_train = calculate_patch_dataset(sections=train_sections, sections_fname=sections_fname, masks_fname=masks_gmwm_fname,\n",
    "                       num_patches=num_train, label_map={0:0, 128:1, 255:2}, patch_size=(268,268))\n",
    "np.savez(\"data/train_gmwm.npz\", X=X_train, Y=Y_train)\n",
    "\n",
    "# sample validation dataset\n",
    "num_val = 20\n",
    "X_val, Y_val = calculate_patch_dataset(sections=val_sections, sections_fname=sections_fname, masks_fname=masks_gmwm_fname,\n",
    "                       num_patches=num_val, label_map={0:0, 128:1, 255:2}, patch_size=(268,268))\n",
    "np.savez(\"data/val_gmwm.npz\", X=X_val, Y=Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bea941dfd234e1aa3ef0480cc3dc49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace68e1fd77f4a22942be91e3aba7421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e4f55247d7464fbbc68479e151f768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24e70c6dee4bceb7d8293fc862ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample training dataset for v1 segmentation\n",
    "num_train = 500\n",
    "X_train, Y_train = calculate_patch_dataset(sections=train_sections, sections_fname=sections_fname, masks_fname=masks_v1_fname,\n",
    "                       num_patches=num_train, label_map={0:0, 255:1}, patch_size=(268,268))\n",
    "np.savez(\"data/train_v1.npz\", X=X_train, Y=Y_train)\n",
    "\n",
    "# sample validation dataset\n",
    "num_val = 20\n",
    "X_val, Y_val = calculate_patch_dataset(sections=val_sections, sections_fname=sections_fname, masks_fname=masks_v1_fname,\n",
    "                       num_patches=num_val, label_map={0:0, 255:1}, patch_size=(268,268))\n",
    "np.savez(\"data/val_v1.npz\", X=X_val, Y=Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b02f6ff58a744b58c345bbb6f581d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f58b84962e14ff994ea96d805a7d822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Calculating sampling_weights\n",
      "Sampling coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7524755a5849098e00d5ce0c80d519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping patches from coordinates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b52baffc6e48f6ae77465fa138ad37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample training dataset for v1 segmentation with 4 classes: bg, gm, wm, v1\n",
    "num_patches = 500\n",
    "label_map = {0:0, 128:1, 255:2, 64:3}\n",
    "patch_size = (268,268)\n",
    "\n",
    "# load images\n",
    "print(\"Loading images\")\n",
    "masks = np.array([imageio.imread(masks_gmwm_fname.format(sec)) for sec in train_sections])\n",
    "masks_v1 = np.array([imageio.imread(masks_v1_fname.format(sec)) for sec in train_sections])\n",
    "masks[masks_v1==255] = 64\n",
    "imgs = np.array([imageio.imread(sections_fname.format(sec)) for sec in train_sections])\n",
    "# ensure equal sampling of all classes\n",
    "print(\"Calculating sampling_weights\")\n",
    "sampling_weights = get_sampling_weights(masks, label_vals=label_map.keys()) \n",
    "# randomly sampled coordinates\n",
    "print(\"Sampling coordinates\")\n",
    "coords = sample_coordinates(imgs, num_patches, patch_size=patch_size, p=sampling_weights)\n",
    "# crop patches\n",
    "print(\"Cropping patches from coordinates\")\n",
    "patches, labels = crop_patches(imgs, masks, coords, patch_size=patch_size, label_map=label_map)\n",
    "\n",
    "# prepare for use as training dataset\n",
    "X_train = np.expand_dims(np.array(patches), 3)\n",
    "Y_train = np.expand_dims(np.array(labels), 3)\n",
    "\n",
    "np.savez(\"data/train_v1gmwm.npz\", X=X_train, Y=Y_train)\n",
    "\n",
    "# sample validation dataset\n",
    "num_patches = 20\n",
    "# load images\n",
    "print(\"Loading images\")\n",
    "masks = np.array([imageio.imread(masks_gmwm_fname.format(sec)) for sec in val_sections])\n",
    "masks_v1 = np.array([imageio.imread(masks_v1_fname.format(sec)) for sec in val_sections])\n",
    "masks[masks_v1==255] = 64\n",
    "imgs = np.array([imageio.imread(sections_fname.format(sec)) for sec in val_sections])\n",
    "# ensure equal sampling of all classes\n",
    "print(\"Calculating sampling_weights\")\n",
    "sampling_weights = get_sampling_weights(masks, label_vals=label_map.keys()) \n",
    "# randomly sampled coordinates\n",
    "print(\"Sampling coordinates\")\n",
    "coords = sample_coordinates(imgs, num_patches, patch_size=patch_size, p=sampling_weights)\n",
    "# crop patches\n",
    "print(\"Cropping patches from coordinates\")\n",
    "patches, labels = crop_patches(imgs, masks, coords, patch_size=patch_size, label_map=label_map)\n",
    "\n",
    "# prepare for use as training dataset\n",
    "X_val = np.expand_dims(np.array(patches), 3)\n",
    "Y_val = np.expand_dims(np.array(labels), 3)\n",
    "\n",
    "np.savez(\"data/val_v1gmwm.npz\", X=X_val, Y=Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
